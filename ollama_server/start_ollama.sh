#!/bin/bash
echo "ğŸš€ Starting Ollama Server..."
echo "=================================="

# Check if Ollama is installed
if ! command -v ollama &> /dev/null; then
    echo "âŒ Ollama not found. Please install it first:"
    echo "curl -fsSL https://ollama.ai/install.sh | sh"
    exit 1
fi

# Check if Ollama is already running
if pgrep -f "ollama serve" > /dev/null; then
    echo "âœ… Ollama server is already running"
    echo "ğŸ”— Server URL: http://127.0.0.1:11434"
    echo "ğŸ“‹ Status: Active"

    # Show running models
    echo ""
    echo "ğŸ“¦ Available models:"
    ollama list
else
    echo "ğŸ”„ Starting Ollama server..."

    # Start Ollama in background
    ollama serve > /dev/null 2>&1 &
    OLLAMA_PID=$!

    # Wait for server to start
    sleep 3

    # Check if server started successfully
    if pgrep -f "ollama serve" > /dev/null; then
        echo "âœ… Ollama server started successfully!"
        echo "ğŸ”— Server URL: http://127.0.0.1:11434"
        echo "ğŸ“‹ Process ID: $OLLAMA_PID"

        # Test connection
        echo ""
        echo "ğŸ§ª Testing connection..."
        if curl -s http://127.0.0.1:11434/api/tags > /dev/null; then
            echo "âœ… Connection test successful"
        else
            echo "âš ï¸  Connection test failed - server may still be starting"
        fi

        echo ""
        echo "ğŸ“¦ Available models:"
        ollama list

        echo ""
        echo "ğŸ¯ Server is ready! You can now:"
        echo "   â€¢ Run search agent: python3 ollama_search_agent.py"
        echo "   â€¢ Stop server: pkill -f 'ollama serve'"

    else
        echo "âŒ Failed to start Ollama server"
        exit 1
    fi
fi

echo ""
echo "ğŸ“Š Server Information:"
echo "   â€¢ Host: 127.0.0.1"
echo "   â€¢ Port: 11434"
echo "   â€¢ API: http://127.0.0.1:11434/api"