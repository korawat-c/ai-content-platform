{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Dynamic LangGraph RAG System\n",
    "\n",
    "**TRULY DYNAMIC VERSION** - No hardcoded queries anywhere!\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… User input drives everything\n",
    "- âœ… State generates queries dynamically\n",
    "- âœ… Each node refines based on previous results\n",
    "- âœ… No pre-programmed questions\n",
    "- âœ… Intelligent query refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment loaded\n",
      "ğŸ“ Working in: /home/oh/Documents/ai-content-platform/notebook\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "import os\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from typing import TypedDict, List, Dict, Any, Optional\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "load_dotenv()\n",
    "print(\"âœ… Environment loaded\")\n",
    "print(f\"ğŸ“ Working in: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic State Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Dynamic state schema - NO HARDCODED QUERIES!\n",
      "ğŸ“‹ User input drives EVERYTHING!\n",
      "ğŸ¤– Human-in-the-loop capability added!\n"
     ]
    }
   ],
   "source": [
    "class DynamicState(TypedDict):\n",
    "    \"\"\"Dynamic state \"\"\"\n",
    "    messages: List[BaseMessage]\n",
    "    \n",
    "    # User input (the ONLY thing that drives the system)\n",
    "    user_question: str\n",
    "    \n",
    "    # Dynamic query generation\n",
    "    current_research_focus: str\n",
    "    generated_queries: List[str]\n",
    "    query_generation_reasoning: List[str]\n",
    "    \n",
    "    # Research results\n",
    "    rag_findings: List[Dict[str, Any]]  # Each with query, results, analysis\n",
    "    web_findings: List[Dict[str, Any]]  # Each with query, results, analysis\n",
    "    \n",
    "    # State-based decisions\n",
    "    information_gaps: List[str]\n",
    "    next_research_strategy: str\n",
    "    research_iteration: int\n",
    "    max_iterations: int\n",
    "    \n",
    "    # Human-in-the-loop additions\n",
    "    human_in_loop: bool\n",
    "    human_queries: List[str]\n",
    "    human_decision: str  # \"continue\" or \"synthesize\"\n",
    "    \n",
    "    # Final synthesis\n",
    "    synthesized_answer: str\n",
    "    confidence_score: float\n",
    "    \n",
    "    # Complete transparency\n",
    "    decision_log: List[str]\n",
    "    state_transitions: List[str]\n",
    "\n",
    "print(\"ğŸ—ï¸ Dynamic state schema - NO HARDCODED QUERIES!\")\n",
    "print(\"ğŸ“‹ User input drives EVERYTHING!\")\n",
    "print(\"ğŸ¤– Human-in-the-loop capability added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oh/miniconda3/envs/py313/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/oh/miniconda3/envs/py313/lib/python3.13/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Setting up document system (IMPROVED for tables)...\n",
      "âœ… Ready: 93 regular chunks = 93 total chunks\n",
      "ğŸ“Š Vector store status: Ready\n"
     ]
    }
   ],
   "source": [
    "# Load and process documents (IMPROVED for better table handling)\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def setup_document_system():\n",
    "    \"\"\"Setup document system once - IMPROVED for tables\"\"\"\n",
    "    print(\"ğŸ“„ Setting up document system (IMPROVED for tables)...\")\n",
    "    \n",
    "    file_path = \"../data/2310.04671v4.pdf\"\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âŒ File not found: {file_path}\")\n",
    "        return None\n",
    "    \n",
    "    # Load and chunk ALL documents\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # IMPROVED: Better chunking for tables - smaller chunks with more overlap\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,  # Smaller chunks for better table coverage\n",
    "        chunk_overlap=400,  # More overlap to catch table content,\n",
    "        # separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]  # Better separators for tables\n",
    "    )\n",
    "    \n",
    "    chunks = splitter.split_documents(docs)\n",
    "    \n",
    "    # IMPROVED: Add table-specific chunks\n",
    "    # table_chunks = []\n",
    "    # for doc in docs:\n",
    "    #     content = doc.page_content\n",
    "    #     # If this looks like a table or contains metrics, create additional chunks\n",
    "    #     if any(keyword in content for keyword in [\"TABLE\"]):\n",
    "    #         # Create smaller chunks around table content\n",
    "    #         lines = content.split('\\n')\n",
    "    #         for i in range(0, len(lines), 5):  # Groups of 5 lines\n",
    "    #             chunk_content = '\\n'.join(lines[i:i+8])  # 8 lines with overlap\n",
    "    #             if len(chunk_content.strip()) > 50:  # Only substantial chunks\n",
    "    #                 table_chunk = Document(page_content=chunk_content, metadata=doc.metadata)\n",
    "    #                 table_chunks.append(table_chunk)\n",
    "    \n",
    "    # Combine regular chunks with table chunks\n",
    "    all_chunks = chunks #+ table_chunks\n",
    "    \n",
    "    # Create vector store\n",
    "    embeddings = OllamaEmbeddings(model=\"dengcao/Qwen3-Embedding-4B:Q4_K_M\")\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"dynamic_rag_improved\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=\"./chroma_dynamic_rag_improved\"\n",
    "    )\n",
    "    \n",
    "    ids = vector_store.add_documents(all_chunks)\n",
    "    \n",
    "    print(f\"âœ… Ready: {len(chunks)} regular chunks = {len(all_chunks)} total chunks\")\n",
    "    return vector_store\n",
    "\n",
    "# Setup once with improved system\n",
    "vector_store = setup_document_system()\n",
    "print(f\"ğŸ“Š Vector store status: {'Ready' if vector_store else 'Failed'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_think_tags(text: str) -> str:\n",
    "    \"\"\"Remove 'Think:' tags from the text.\"\"\"\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    return ''.join(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Query Generation Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ PURELY DYNAMIC query generator ready - NO hardcoded terms!\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.tools import tool\n",
    "from ollama import Client\n",
    "import re\n",
    "\n",
    "# Initialize LLM and client\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen3:8b\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")\n",
    "\n",
    "ollama_api_key = os.getenv('OLLAMA_API_KEY')\n",
    "client = Client(\n",
    "    host='http://127.0.0.1:11434',\n",
    "    headers={'Authorization': f'Bearer {ollama_api_key}'}\n",
    ") if ollama_api_key else None\n",
    "\n",
    "def clean_llm_response(text: str) -> str:\n",
    "    \"\"\"Clean LLM thinking artifacts from response\"\"\"\n",
    "    cleaned = re.sub(r\"</think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    return cleaned.strip()\n",
    "\n",
    "@tool\n",
    "def generate_research_queries(user_question: str, previous_findings: str = \"\", iteration: int = 1) -> str:\n",
    "    \"\"\"\n",
    "    TRULY DYNAMIC query generation - NO hardcoded terms whatsoever.\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§  Generating queries for: '{user_question}' (iteration {iteration})\")\n",
    "    \n",
    "    if iteration == 1:\n",
    "        # PURELY dynamic - no hardcoded terms\n",
    "        prompt = f\"\"\"Generate 3-4 specific search queries to research: \"{user_question}\"\n",
    "\n",
    "Rules:\n",
    "1. Create different variations of the original question\n",
    "2. Break down complex questions into simpler search terms\n",
    "3. Use synonyms and related concepts naturally\n",
    "4. Each query should be on a new line\n",
    "5. No explanations, just queries\n",
    "\n",
    "Example approach:\n",
    "- Original question rephrased\n",
    "- Key terms from question combined\n",
    "- Broader/related concepts\"\"\"\n",
    "    else:\n",
    "        # Dynamic refinement based on findings\n",
    "        prompt = f\"\"\"Based on previous research, generate 2-3 refined search queries for: \"{user_question}\"\n",
    "\n",
    "Previous findings:\n",
    "{previous_findings[:1000]}...\n",
    "\n",
    "Generate queries to:\n",
    "1. Fill information gaps identified\n",
    "2. Explore different angles\n",
    "3. Get more specific details\n",
    "\n",
    "Format: One query per line, no explanations.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        # Clean LLM thinking artifacts\n",
    "        cleaned_content = remove_think_tags(response.content)\n",
    "        queries = [q.strip() for q in cleaned_content.split('\\n') if q.strip()]\n",
    "        \n",
    "        # NO hardcoded additions - purely dynamic\n",
    "        print(f\"ğŸ¯ Generated {len(queries)} truly dynamic queries:\")\n",
    "        for i, query in enumerate(queries, 1):\n",
    "            print(f\"   {i}. {query}\")\n",
    "        \n",
    "        return \"\\n\".join(queries)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Query generation error: {e}\")\n",
    "        return user_question  # Fallback to original question\n",
    "\n",
    "print(\"ğŸ› ï¸ PURELY DYNAMIC query generator ready - NO hardcoded terms!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ Dynamic research tools + Human-in-the-loop ready\n"
     ]
    }
   ],
   "source": [
    "# RAG Research Tool\n",
    "@tool\n",
    "def execute_rag_research(queries: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute RAG research using the vector store with generated queries.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ” Executing RAG research with queries:\")\n",
    "    query_list = [q.strip() for q in queries.split('\\n') if q.strip()]\n",
    "    for i, query in enumerate(query_list, 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    if not vector_store:\n",
    "        return \"Error: Vector store not available\"\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for query in query_list:\n",
    "        try:\n",
    "            # Search the vector store\n",
    "            docs = vector_store.similarity_search(query, k=5)\n",
    "            \n",
    "            # Format results\n",
    "            query_results = f\"\\nQuery: {query}\\n\"\n",
    "            query_results += \"=\"*50 + \"\\n\"\n",
    "            \n",
    "            for i, doc in enumerate(docs, 1):\n",
    "                query_results += f\"Document {i}:\\n\"\n",
    "                query_results += f\"Content: {doc.page_content}\\n\"\n",
    "                query_results += f\"Metadata: {doc.metadata}\\n\"\n",
    "                query_results += \"-\"*30 + \"\\n\"\n",
    "            \n",
    "            all_results.append(query_results)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error searching for query '{query}': {e}\"\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "            all_results.append(error_msg)\n",
    "    \n",
    "    combined_results = \"\\n\".join(all_results)\n",
    "    print(f\"âœ… RAG research complete: {len(combined_results)} characters\")\n",
    "    return combined_results\n",
    "\n",
    "# Web Research Tool  \n",
    "@tool\n",
    "def execute_web_research(queries: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute web research for current/recent information.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸŒ Executing web research with queries:\")\n",
    "    query_list = [q.strip() for q in queries.split('\\n') if q.strip()]\n",
    "    \n",
    "    # Limit web queries to avoid excessive API calls\n",
    "    web_queries = query_list[:2]\n",
    "    for i, query in enumerate(web_queries, 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    for query in web_queries:\n",
    "        try:\n",
    "            # For now, simulate web search with a focused response\n",
    "            # In a real implementation, you would use a web search API\n",
    "            web_result = f\"\\nWeb Search Query: {query}\\n\"\n",
    "            web_result += \"=\"*50 + \"\\n\"\n",
    "            web_result += \"Note: Web search simulation - replace with actual web search API\\n\"\n",
    "            web_result += f\"Would search for current information about: {query}\\n\"\n",
    "            web_result += \"This could include recent developments, news, or updates\\n\"\n",
    "            \n",
    "            all_results.append(web_result)\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error in web search for query '{query}': {e}\"\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "            all_results.append(error_msg)\n",
    "    \n",
    "    combined_results = \"\\n\".join(all_results)\n",
    "    print(f\"âœ… Web research complete: {len(combined_results)} characters\")\n",
    "    return combined_results\n",
    "\n",
    "@tool\n",
    "def analyze_research_findings(findings: str, original_question: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze research findings to identify gaps and next steps.\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š Analyzing research findings for: '{original_question}'\")\n",
    "    \n",
    "    prompt = f\"\"\"Analyze these research findings for the question: \"{original_question}\"\n",
    "\n",
    "Research Findings:\n",
    "{findings[:2000]}...\n",
    "\n",
    "Provide analysis in three parts:\n",
    "1. KEY_INFORMATION_FOUND: List the main information discovered\n",
    "2. INFORMATION_GAPS: What important details are still missing?\n",
    "3. NEXT_RESEARCH_FOCUS: What specific aspects need more research?\n",
    "\n",
    "Be specific and actionable.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        # Clean LLM thinking artifacts\n",
    "        cleaned_content = clean_llm_response(response.content)\n",
    "        print(f\"ğŸ“ˆ Analysis complete: {len(cleaned_content)} characters\")\n",
    "        return cleaned_content\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Analysis error: {e}\")\n",
    "        return f\"Analysis failed: {e}\"\n",
    "\n",
    "# Human-in-the-loop interaction tools\n",
    "def display_research_summary(state: DynamicState) -> str:\n",
    "    \"\"\"Display a comprehensive summary of all research done so far\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¤– HUMAN-IN-THE-LOOP: RESEARCH SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Original Question: {state['user_question']}\")\n",
    "    print(f\"ğŸ”„ Current Iteration: {state['research_iteration']}\")\n",
    "    print(f\"ğŸ“Š Total Research Iterations: {state['research_iteration'] - 1}\")\n",
    "    \n",
    "    # Show all queries tried\n",
    "    all_queries = []\n",
    "    for finding in state['rag_findings']:\n",
    "        iteration = finding['iteration']\n",
    "        for query in finding['queries']:\n",
    "            all_queries.append(f\"Iteration {iteration}: {query}\")\n",
    "    \n",
    "    print(f\"\\nğŸ” ALL QUERIES TRIED ({len(all_queries)} total):\")\n",
    "    for i, query in enumerate(all_queries, 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    # Show research results summary\n",
    "    print(f\"\\nğŸ“‹ RESEARCH RESULTS SUMMARY:\")\n",
    "    \n",
    "    # RAG findings\n",
    "    if state['rag_findings']:\n",
    "        print(f\"\\nğŸ“š RAG Research Results:\")\n",
    "        total_rag_chars = sum(len(f['results']) for f in state['rag_findings'])\n",
    "        print(f\"   â€¢ Total iterations: {len(state['rag_findings'])}\")\n",
    "        print(f\"   â€¢ Total characters: {total_rag_chars}\")\n",
    "        \n",
    "        for i, finding in enumerate(state['rag_findings'], 1):\n",
    "            print(f\"   \\n   ğŸ“„ Iteration {finding['iteration']}:\")\n",
    "            print(f\"      Queries: {len(finding['queries'])}\")\n",
    "            print(f\"      Results: {len(finding['results'])} characters\")\n",
    "            # Show brief preview\n",
    "            preview = finding['summary'][:200] + \"...\" if len(finding['summary']) > 200 else finding['summary']\n",
    "            print(f\"      Preview: {preview}\")\n",
    "    \n",
    "    # Web findings\n",
    "    if state['web_findings']:\n",
    "        print(f\"\\nğŸŒ Web Research Results:\")\n",
    "        total_web_chars = sum(len(f['results']) for f in state['web_findings'])\n",
    "        print(f\"   â€¢ Total iterations: {len(state['web_findings'])}\")\n",
    "        print(f\"   â€¢ Total characters: {total_web_chars}\")\n",
    "        \n",
    "        for i, finding in enumerate(state['web_findings'], 1):\n",
    "            print(f\"   \\n   ğŸŒ Iteration {finding['iteration']}:\")\n",
    "            print(f\"      Queries: {len(finding['queries'])}\")\n",
    "            print(f\"      Results: {len(finding['results'])} characters\")\n",
    "            # Show brief preview\n",
    "            preview = finding['summary'][:200] + \"...\" if len(finding['summary']) > 200 else finding['summary']\n",
    "            print(f\"      Preview: {preview}\")\n",
    "    \n",
    "    # Show information gaps\n",
    "    if state['information_gaps']:\n",
    "        print(f\"\\nğŸ•³ï¸ IDENTIFIED INFORMATION GAPS:\")\n",
    "        for gap in state['information_gaps']:\n",
    "            print(f\"   â€¢ {gap}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… NO SPECIFIC GAPS IDENTIFIED\")\n",
    "    \n",
    "    # Show human queries already added\n",
    "    if state['human_queries']:\n",
    "        print(f\"\\nğŸ‘¤ HUMAN-ADDED QUERIES:\")\n",
    "        for i, query in enumerate(state['human_queries'], 1):\n",
    "            print(f\"   {i}. {query}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    return \"Research summary displayed\"\n",
    "\n",
    "def get_human_input(state: DynamicState) -> str:\n",
    "    \"\"\"\n",
    "    Interactive function to get human input during research\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ¤– HUMAN-IN-THE-LOOP INTERACTION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Research Status:\")\n",
    "    print(f\"   â€¢ Iteration: {state['research_iteration']}/{state['max_iterations']}\")\n",
    "    print(f\"   â€¢ Total queries tried: {len(state['generated_queries']) + len(state['human_queries'])}\")\n",
    "    print(f\"   â€¢ Research complete: {len(state['rag_findings']) + len(state['web_findings'])} findings\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ YOUR OPTIONS:\")\n",
    "    print(f\"   1. Add specific search queries (comma-separated)\")\n",
    "    print(f\"   2. Type 'synthesize' to proceed to final synthesis\")\n",
    "    print(f\"   3. Type 'continue' for automatic research continuation\")\n",
    "    print(f\"   4. Type 'help' for more options\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(f\"\\nğŸ‘¤ Your choice: \").strip()\n",
    "        \n",
    "        if user_input.lower() == 'synthesize':\n",
    "            return \"synthesize\"\n",
    "        elif user_input.lower() == 'continue':\n",
    "            return \"continue\"\n",
    "        elif user_input.lower() == 'help':\n",
    "            print(f\"\\nğŸ“– DETAILED OPTIONS:\")\n",
    "            print(f\"   â€¢ Add queries: 'query1, query2, query3'\")\n",
    "            print(f\"   â€¢ Synthesize: 'synthesize' - create final answer now\")\n",
    "            print(f\"   â€¢ Continue: 'continue' - let AI continue automatically\")\n",
    "            print(f\"   â€¢ Status: 'status' - show detailed research status\")\n",
    "            print(f\"   â€¢ Preview: 'preview' - show current research preview\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'status':\n",
    "            print(f\"\\nğŸ“Š DETAILED STATUS:\")\n",
    "            print(f\"   â€¢ RAG findings: {len(state['rag_findings'])}\")\n",
    "            print(f\"   â€¢ Web findings: {len(state['web_findings'])}\")\n",
    "            print(f\"   â€¢ Information gaps: {len(state['information_gaps'])}\")\n",
    "            print(f\"   â€¢ Human queries: {len(state['human_queries'])}\")\n",
    "            continue\n",
    "        elif user_input.lower() == 'preview':\n",
    "            # Show a preview of current findings\n",
    "            all_findings = \"\\n\".join([f[\"summary\"] for f in state.get(\"rag_findings\", []) + state.get(\"web_findings\", [])])\n",
    "            print(f\"\\nğŸ“‹ CURRENT RESEARCH PREVIEW:\")\n",
    "            print(f\"   {all_findings[:1000]}...\")\n",
    "            continue\n",
    "        else:\n",
    "            # Assume it's a query input\n",
    "            queries = [q.strip() for q in user_input.split(',') if q.strip()]\n",
    "            if queries:\n",
    "                return f\"queries: {', '.join(queries)}\"\n",
    "            else:\n",
    "                print(\"âŒ Invalid input. Please try again.\")\n",
    "                continue\n",
    "\n",
    "print(\"ğŸ› ï¸ Dynamic research tools + Human-in-the-loop ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DYNAMIC LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Building DYNAMIC LangGraph workflow...\n",
      "âœ… DYNAMIC workflow with human-in-the-loop compiled (TESTING MODE - triggers after 1 iteration)\n",
      "\n",
      "ğŸš€ DYNAMIC LangGraph workflow with HUMAN-IN-THE-LOOP ready!\n",
      "ğŸ“‹ User input drives EVERYTHING - no hardcoded queries!\n",
      "ğŸ¤– Human interaction NOW triggers after 1 iteration for testing!\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "def build_dynamic_workflow():\n",
    "    \"\"\"Build TRULY DYNAMIC workflow - no hardcoded queries!\"\"\"\n",
    "    print(\"ğŸ”§ Building DYNAMIC LangGraph workflow...\")\n",
    "    \n",
    "    workflow = StateGraph(DynamicState)\n",
    "    \n",
    "    # Node 1: Dynamic Query Generation\n",
    "    def query_generation_node(state: DynamicState) -> DynamicState:\n",
    "        \"\"\"Generate queries DYNAMICALLY based on user question and state\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ§  NODE 1: DYNAMIC QUERY GENERATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        iteration = state[\"research_iteration\"]\n",
    "        user_question = state[\"user_question\"]\n",
    "        previous_findings = \"\\n\".join([f[\"summary\"] for f in state.get(\"rag_findings\", []) + state.get(\"web_findings\", [])])\n",
    "        \n",
    "        print(f\"ğŸ¯ User Question: {user_question}\")\n",
    "        print(f\"ğŸ”„ Research Iteration: {iteration}\")\n",
    "        print(f\"ğŸ“Š Previous Findings: {len(previous_findings)} characters\")\n",
    "        \n",
    "        # Check if we have human queries to process\n",
    "        if state[\"human_queries\"] and iteration > 1:\n",
    "            # Use human-added queries\n",
    "            human_query_text = \"\\n\".join(state[\"human_queries\"])\n",
    "            state[\"generated_queries\"] = state[\"human_queries\"].copy()\n",
    "            state[\"human_queries\"] = []  # Clear after using\n",
    "            reasoning = f\"Iteration {iteration}: Using human-added queries: {human_query_text}\"\n",
    "        else:\n",
    "            # Generate DYNAMIC queries\n",
    "            generated_queries = generate_research_queries.invoke({\n",
    "                \"user_question\": user_question,\n",
    "                \"previous_findings\": previous_findings,\n",
    "                \"iteration\": iteration\n",
    "            })\n",
    "            \n",
    "            query_list = [q.strip() for q in generated_queries.split('\\n') if q.strip()]\n",
    "            state[\"generated_queries\"] = query_list\n",
    "            \n",
    "            # Log the reasoning\n",
    "            reasoning = f\"Iteration {iteration}: Generated {len(query_list)} queries based on user question and previous findings\"\n",
    "        \n",
    "        state[\"query_generation_reasoning\"].append(reasoning)\n",
    "        state[\"decision_log\"].append(f\"Generated {len(state['generated_queries'])} queries for iteration {iteration}\")\n",
    "        state[\"state_transitions\"].append(\"query_generation -> research_execution\")\n",
    "        \n",
    "        print(f\"âœ… Generated/Used {len(state['generated_queries'])} queries\")\n",
    "        return state\n",
    "    \n",
    "    # Node 2: Research Execution\n",
    "    def research_execution_node(state: DynamicState) -> DynamicState:\n",
    "        \"\"\"Execute research using DYNAMICALLY generated queries\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ” NODE 2: DYNAMIC RESEARCH EXECUTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        queries = \"\\n\".join(state[\"generated_queries\"])\n",
    "        \n",
    "        print(f\"ğŸ” Executing research with {len(state['generated_queries'])} queries\")\n",
    "        \n",
    "        # Execute RAG research\n",
    "        rag_results = execute_rag_research.invoke({\"queries\": queries})\n",
    "        rag_finding = {\n",
    "            \"iteration\": state[\"research_iteration\"],\n",
    "            \"queries\": state[\"generated_queries\"].copy(),\n",
    "            \"results\": rag_results,\n",
    "            \"summary\": rag_results[:500] + \"...\" if len(rag_results) > 500 else rag_results\n",
    "        }\n",
    "        state[\"rag_findings\"].append(rag_finding)\n",
    "        \n",
    "        # # Execute web research (for current/recent info)\n",
    "        # web_results = execute_web_research.invoke({\"queries\": queries})\n",
    "        # web_finding = {\n",
    "        #     \"iteration\": state[\"research_iteration\"],\n",
    "        #     \"queries\": state[\"generated_queries\"][:2],  # Limit web queries\n",
    "        #     \"results\": web_results,\n",
    "        #     \"summary\": web_results[:500] + \"...\" if len(web_results) > 500 else web_results\n",
    "        # }\n",
    "        # state[\"web_findings\"].append(web_finding)\n",
    "        \n",
    "        # Update state\n",
    "        state[\"decision_log\"].append(f\"Executed research with {len(state['generated_queries'])} queries\")\n",
    "        state[\"state_transitions\"].append(\"research_execution -> analysis\")\n",
    "        \n",
    "        print(f\"âœ… Research execution complete\")\n",
    "        return state\n",
    "    \n",
    "    # Node 3: Analysis and Decision (with human-in-the-loop trigger)\n",
    "    def analysis_decision_node(state: DynamicState) -> DynamicState:\n",
    "        \"\"\"Analyze findings and decide on next steps - triggers human input after 1 try for testing\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“Š NODE 3: ANALYSIS & DECISION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Combine all findings\n",
    "        all_findings = \"\\n\".join([\n",
    "            f[\"summary\"] for f in state.get(\"rag_findings\", []) + state.get(\"web_findings\", [])\n",
    "        ])\n",
    "        \n",
    "        print(f\"ğŸ“ˆ Analyzing {len(all_findings)} characters of findings\")\n",
    "        \n",
    "        # Analyze findings\n",
    "        analysis = analyze_research_findings.invoke({\n",
    "            \"findings\": all_findings,\n",
    "            \"original_question\": state[\"user_question\"]\n",
    "        })\n",
    "        \n",
    "        # Extract gaps and decide next steps\n",
    "        gaps = []\n",
    "        if \"INFORMATION_GAPS:\" in analysis:\n",
    "            gaps_section = analysis.split(\"INFORMATION_GAPS:\")[1].split(\"NEXT_RESEARCH_FOCUS:\")[0] if \"NEXT_RESEARCH_FOCUS:\" in analysis else analysis.split(\"INFORMATION_GAPS:\")[1]\n",
    "            gaps = [gap.strip() for gap in gaps_section.split('\\n') if gap.strip() and not gap.startswith('-')]\n",
    "        \n",
    "        state[\"information_gaps\"] = gaps\n",
    "        \n",
    "        # Decision logic with human-in-the-loop trigger - MODIFIED FOR TESTING\n",
    "        current_iteration = state[\"research_iteration\"]\n",
    "        max_iterations = state[\"max_iterations\"]\n",
    "        \n",
    "        # TESTING: Trigger human-in-the-loop after 1 try instead of 3\n",
    "        if current_iteration >= 1:  # Changed from 3 to 1 for testing\n",
    "            state[\"human_in_loop\"] = True\n",
    "            decision = \"human_interaction\"\n",
    "            reason = f\"ğŸ§ª TESTING: Triggering human-in-the-loop after {current_iteration} iteration (set to trigger after 1 for testing)\"\n",
    "        elif len(gaps) == 0 or \"complete\" in analysis.lower() or \"comprehensive\" in analysis.lower():\n",
    "            decision = \"synthesize\"\n",
    "            reason = \"Research appears complete\"\n",
    "        else:\n",
    "            decision = \"continue\"\n",
    "            reason = f\"Information gaps identified: {len(gaps)} gaps\"\n",
    "        \n",
    "        state[\"next_research_strategy\"] = decision\n",
    "        state[\"decision_log\"].append(f\"Decision: {decision} - {reason}\")\n",
    "        \n",
    "        if decision == \"continue\":\n",
    "            state[\"research_iteration\"] += 1\n",
    "            state[\"state_transitions\"].append(\"analysis -> query_generation (continue)\")\n",
    "            print(f\"ğŸ”„ Continuing research - {reason}\")\n",
    "        elif decision == \"human_interaction\":\n",
    "            state[\"state_transitions\"].append(\"analysis -> human_interaction\")\n",
    "            print(f\"ğŸ¤– {reason}\")\n",
    "        else:\n",
    "            state[\"state_transitions\"].append(\"analysis -> synthesis\")\n",
    "            print(f\"ğŸ¯ Research complete - {reason}\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    # Node 4: Human Interaction\n",
    "    def human_interaction_node(state: DynamicState) -> DynamicState:\n",
    "        \"\"\"Human-in-the-loop interaction node\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ¤– NODE 4: HUMAN-IN-THE-LOOP INTERACTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Display comprehensive research summary\n",
    "        display_research_summary(state)\n",
    "        \n",
    "        # Get human input\n",
    "        human_choice = get_human_input(state)\n",
    "        \n",
    "        # Process human decision\n",
    "        if human_choice == \"synthesize\":\n",
    "            state[\"human_decision\"] = \"synthesize\"\n",
    "            state[\"next_research_strategy\"] = \"synthesize\"\n",
    "            state[\"decision_log\"].append(\"Human chose to synthesize final answer\")\n",
    "            state[\"state_transitions\"].append(\"human_interaction -> synthesis\")\n",
    "            print(f\"ğŸ¯ Human decided to synthesize\")\n",
    "            \n",
    "        elif human_choice == \"continue\":\n",
    "            state[\"human_decision\"] = \"continue\"\n",
    "            state[\"research_iteration\"] += 1\n",
    "            state[\"next_research_strategy\"] = \"continue\"\n",
    "            state[\"decision_log\"].append(\"Human chose to continue automatic research\")\n",
    "            state[\"state_transitions\"].append(\"human_interaction -> query_generation\")\n",
    "            print(f\"ğŸ”„ Human decided to continue automatically\")\n",
    "            \n",
    "        elif human_choice.startswith(\"queries:\"):\n",
    "            # Extract queries from human input\n",
    "            queries_text = human_choice.replace(\"queries:\", \"\").strip()\n",
    "            human_queries = [q.strip() for q in queries_text.split(',') if q.strip()]\n",
    "            \n",
    "            state[\"human_queries\"] = human_queries\n",
    "            state[\"research_iteration\"] += 1\n",
    "            state[\"next_research_strategy\"] = \"human_queries\"\n",
    "            state[\"decision_log\"].append(f\"Human added {len(human_queries)} specific queries\")\n",
    "            state[\"state_transitions\"].append(\"human_interaction -> query_generation\")\n",
    "            print(f\"ğŸ‘¤ Human added {len(human_queries)} queries\")\n",
    "            \n",
    "        else:\n",
    "            # Default to continue\n",
    "            state[\"human_decision\"] = \"continue\"\n",
    "            state[\"research_iteration\"] += 1\n",
    "            state[\"next_research_strategy\"] = \"continue\"\n",
    "            state[\"decision_log\"].append(\"Human interaction processed - continuing\")\n",
    "            state[\"state_transitions\"].append(\"human_interaction -> query_generation\")\n",
    "            print(f\"ğŸ”„ Processing human input - continuing\")\n",
    "        \n",
    "        state[\"human_in_loop\"] = False  # Reset flag\n",
    "        return state\n",
    "    \n",
    "    # Node 5: Synthesis\n",
    "    def synthesis_node(state: DynamicState) -> DynamicState:\n",
    "        \"\"\"Synthesize final answer from all research\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ¯ NODE 5: SYNTHESIS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Combine all research\n",
    "        all_research = \"\\n\".join([\n",
    "            f\"RAG Research (Iteration {f['iteration']}):\\n{f['results']}\\n\"\n",
    "            for f in state.get(\"rag_findings\", [])\n",
    "        ] + [\n",
    "            f\"Web Research (Iteration {f['iteration']}):\\n{f['results']}\\n\"\n",
    "            for f in state.get(\"web_findings\", [])\n",
    "        ])\n",
    "        \n",
    "        print(f\"ğŸ“ Synthesizing {len(all_research)} characters of research\")\n",
    "        \n",
    "        # Synthesize final answer\n",
    "        synthesis_prompt = f\"\"\"Based on comprehensive research, provide a complete answer to:\n",
    "\n",
    "\"{state['user_question']}\"\n",
    "\n",
    "Research Data:\n",
    "{all_research}\n",
    "\n",
    "Provide a comprehensive, well-structured answer that:\n",
    "1. Directly addresses the user's question\n",
    "2. Incorporates all relevant findings from the research\n",
    "3. Is clear, specific, and informative\n",
    "4. Includes specific details and examples when available\"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(synthesis_prompt)\n",
    "            # Clean LLM thinking artifacts from synthesis\n",
    "            final_answer = clean_llm_response(response.content)\n",
    "            \n",
    "            state[\"synthesized_answer\"] = final_answer\n",
    "            state[\"confidence_score\"] = 0.8  # Could be calculated dynamically\n",
    "            state[\"decision_log\"].append(\"Final answer synthesized\")\n",
    "            state[\"state_transitions\"].append(\"synthesis -> end\")\n",
    "            \n",
    "            print(f\"âœ… Synthesis complete: {len(final_answer)} characters\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Synthesis failed: {e}\"\n",
    "            state[\"synthesized_answer\"] = error_msg\n",
    "            state[\"confidence_score\"] = 0.0\n",
    "            print(f\"âŒ {error_msg}\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"query_generation\", query_generation_node)\n",
    "    workflow.add_node(\"research_execution\", research_execution_node)\n",
    "    workflow.add_node(\"analysis_decision\", analysis_decision_node)\n",
    "    workflow.add_node(\"human_interaction\", human_interaction_node)\n",
    "    workflow.add_node(\"synthesis\", synthesis_node)\n",
    "    \n",
    "    # Define conditional edges\n",
    "    def decide_next_step(state: DynamicState) -> str:\n",
    "        \"\"\"Decide next step based on research strategy\"\"\"\n",
    "        if state[\"next_research_strategy\"] == \"continue\":\n",
    "            return \"query_generation\"  # Continue researching\n",
    "        elif state[\"next_research_strategy\"] == \"human_queries\":\n",
    "            return \"query_generation\"  # Use human queries\n",
    "        elif state[\"next_research_strategy\"] == \"human_interaction\":\n",
    "            return \"human_interaction\"  # Get human input\n",
    "        else:\n",
    "            return \"synthesis\"  # Move to final synthesis\n",
    "    \n",
    "    # Define edges\n",
    "    workflow.set_entry_point(\"query_generation\")\n",
    "    workflow.add_edge(\"query_generation\", \"research_execution\")\n",
    "    workflow.add_edge(\"research_execution\", \"analysis_decision\")\n",
    "    workflow.add_conditional_edges(\n",
    "        \"analysis_decision\",\n",
    "        decide_next_step,\n",
    "        {\n",
    "            \"query_generation\": \"query_generation\",\n",
    "            \"human_interaction\": \"human_interaction\",\n",
    "            \"synthesis\": \"synthesis\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_interaction\",\n",
    "        decide_next_step,\n",
    "        {\n",
    "            \"query_generation\": \"query_generation\",\n",
    "            \"synthesis\": \"synthesis\"\n",
    "        }\n",
    "    )\n",
    "    workflow.add_edge(\"synthesis\", END)\n",
    "    \n",
    "    # Compile workflow\n",
    "    app = workflow.compile()\n",
    "    print(\"âœ… DYNAMIC workflow with human-in-the-loop compiled (TESTING MODE - triggers after 1 iteration)\")\n",
    "    \n",
    "    return app\n",
    "\n",
    "# Build the dynamic workflow with human-in-the-loop\n",
    "dynamic_app = build_dynamic_workflow()\n",
    "print(\"\\nğŸš€ DYNAMIC LangGraph workflow with HUMAN-IN-THE-LOOP ready!\")\n",
    "print(\"ğŸ“‹ User input drives EVERYTHING - no hardcoded queries!\")\n",
    "print(\"ğŸ¤– Human interaction NOW triggers after 1 iteration for testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic_app"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the DYNAMIC System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "ğŸš€ STARTING DYNAMIC LANGGRAPH RESEARCH WITH HUMAN-IN-THE-LOOP\n",
      "================================================================================\n",
      "ğŸ¯ User Question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "ğŸ”„ Max Iterations: 2\n",
      "ğŸ“‹ Queries generated DYNAMICALLY - no hardcoding!\n",
      "ğŸ¤– Human interaction triggered after 3 iterations!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Initial State:\n",
      "   User Question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "   Max Iterations: 2\n",
      "   Strategy: FULLY DYNAMIC with HUMAN-IN-THE-LOOP\n",
      "\n",
      "ğŸ”„ Executing DYNAMIC workflow with human interaction...\n",
      "\n",
      "============================================================\n",
      "ğŸ§  NODE 1: DYNAMIC QUERY GENERATION\n",
      "============================================================\n",
      "ğŸ¯ User Question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "ğŸ”„ Research Iteration: 1\n",
      "ğŸ“Š Previous Findings: 0 characters\n",
      "\n",
      "ğŸ§  Generating queries for: 'Tell me the comparison results between GPT-4V and ours model and give an example as well.' (iteration 1)\n",
      "ğŸ¯ Generated 4 truly dynamic queries:\n",
      "   1. What are the performance differences between GPT-4V and our AI model?\n",
      "   2. How does GPT-4V compare to our model in terms of image recognition and text generation?\n",
      "   3. What are the key capabilities and limitations of GPT-4V versus our model, with an example?\n",
      "   4. Can you provide a real-world example comparing GPT-4V's performance with our model in a specific task?\n",
      "âœ… Generated/Used 4 queries\n",
      "\n",
      "============================================================\n",
      "ğŸ” NODE 2: DYNAMIC RESEARCH EXECUTION\n",
      "============================================================\n",
      "ğŸ” Executing research with 4 queries\n",
      "ğŸ” Executing RAG research with queries:\n",
      "   1. What are the performance differences between GPT-4V and our AI model?\n",
      "   2. How does GPT-4V compare to our model in terms of image recognition and text generation?\n",
      "   3. What are the key capabilities and limitations of GPT-4V versus our model, with an example?\n",
      "   4. Can you provide a real-world example comparing GPT-4V's performance with our model in a specific task?\n",
      "âœ… RAG research complete: 45059 characters\n",
      "âœ… Research execution complete\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š NODE 3: ANALYSIS & DECISION\n",
      "============================================================\n",
      "ğŸ“ˆ Analyzing 503 characters of findings\n",
      "ğŸ“Š Analyzing research findings for: 'Tell me the comparison results between GPT-4V and ours model and give an example as well.'\n",
      "ğŸ“ˆ Analysis complete: 5340 characters\n",
      "ğŸ¤– ğŸ§ª TESTING: Triggering human-in-the-loop after 1 iteration (set to trigger after 1 for testing)\n",
      "\n",
      "============================================================\n",
      "ğŸ¤– NODE 4: HUMAN-IN-THE-LOOP INTERACTION\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ¤– HUMAN-IN-THE-LOOP: RESEARCH SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ Original Question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "ğŸ”„ Current Iteration: 1\n",
      "ğŸ“Š Total Research Iterations: 0\n",
      "\n",
      "ğŸ” ALL QUERIES TRIED (4 total):\n",
      "   1. Iteration 1: What are the performance differences between GPT-4V and our AI model?\n",
      "   2. Iteration 1: How does GPT-4V compare to our model in terms of image recognition and text generation?\n",
      "   3. Iteration 1: What are the key capabilities and limitations of GPT-4V versus our model, with an example?\n",
      "   4. Iteration 1: Can you provide a real-world example comparing GPT-4V's performance with our model in a specific task?\n",
      "\n",
      "ğŸ“‹ RESEARCH RESULTS SUMMARY:\n",
      "\n",
      "ğŸ“š RAG Research Results:\n",
      "   â€¢ Total iterations: 1\n",
      "   â€¢ Total characters: 45059\n",
      "   \n",
      "   ğŸ“„ Iteration 1:\n",
      "      Queries: 4\n",
      "      Results: 45059 characters\n",
      "      Preview: \n",
      "Query: What are the performance differences between GPT-4V and our AI model?\n",
      "==================================================\n",
      "Document 1:\n",
      "Content: respect.\n",
      "The final observation is about the perfor...\n",
      "\n",
      "âœ… NO SPECIFIC GAPS IDENTIFIED\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ¤– HUMAN-IN-THE-LOOP INTERACTION\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š Research Status:\n",
      "   â€¢ Iteration: 1/2\n",
      "   â€¢ Total queries tried: 4\n",
      "   â€¢ Research complete: 1 findings\n",
      "\n",
      "ğŸ¯ YOUR OPTIONS:\n",
      "   1. Add specific search queries (comma-separated)\n",
      "   2. Type 'synthesize' to proceed to final synthesis\n",
      "   3. Type 'continue' for automatic research continuation\n",
      "   4. Type 'help' for more options\n",
      "ğŸ¯ Human decided to synthesize\n",
      "\n",
      "============================================================\n",
      "ğŸ¯ NODE 5: SYNTHESIS\n",
      "============================================================\n",
      "ğŸ“ Synthesizing 45088 characters of research\n",
      "âœ… Synthesis complete: 8499 characters\n"
     ]
    }
   ],
   "source": [
    "async def run_dynamic_research(user_question: str, max_iterations: int = 3):\n",
    "    \"\"\"Run the DYNAMIC research system with HUMAN-IN-THE-LOOP\"\"\"\n",
    "    print(\"ğŸš€ STARTING DYNAMIC LANGGRAPH RESEARCH WITH HUMAN-IN-THE-LOOP\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ğŸ¯ User Question: {user_question}\")\n",
    "    print(f\"ğŸ”„ Max Iterations: {max_iterations}\")\n",
    "    print(\"ğŸ“‹ Queries generated DYNAMICALLY - no hardcoding!\")\n",
    "    print(\"ğŸ¤– Human interaction triggered after 3 iterations!\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    if not vector_store:\n",
    "        print(\"âŒ Cannot run - vector store not available\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize DYNAMIC state with human-in-the-loop fields\n",
    "    initial_state = DynamicState(\n",
    "        messages=[SystemMessage(content=\"You are an expert research assistant that dynamically generates queries based on user input and research findings.\")],\n",
    "        user_question=user_question,\n",
    "        current_research_focus=\"\",\n",
    "        generated_queries=[],\n",
    "        query_generation_reasoning=[],\n",
    "        rag_findings=[],\n",
    "        web_findings=[],\n",
    "        information_gaps=[],\n",
    "        next_research_strategy=\"\",\n",
    "        research_iteration=1,\n",
    "        max_iterations=max_iterations,\n",
    "        synthesized_answer=\"\",\n",
    "        confidence_score=0.0,\n",
    "        decision_log=[],\n",
    "        state_transitions=[],\n",
    "        # Human-in-the-loop fields\n",
    "        human_in_loop=False,\n",
    "        human_queries=[],\n",
    "        human_decision=\"\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Initial State:\")\n",
    "    print(f\"   User Question: {user_question}\")\n",
    "    print(f\"   Max Iterations: {max_iterations}\")\n",
    "    print(f\"   Strategy: FULLY DYNAMIC with HUMAN-IN-THE-LOOP\")\n",
    "    \n",
    "    # Run the DYNAMIC workflow\n",
    "    print(f\"\\nğŸ”„ Executing DYNAMIC workflow with human interaction...\")\n",
    "    final_state = await dynamic_app.ainvoke(initial_state)\n",
    "    \n",
    "    return final_state\n",
    "\n",
    "# Test with different questions\n",
    "test_questions = [\n",
    "    \"Tell me the comparison results between GPT-4V and ours model and give an example as well.\"\n",
    "]\n",
    "\n",
    "# Let's test with the question\n",
    "test_question = test_questions[0]  # \n",
    "print(f\"ğŸ§ª Testing question: {test_question}\")\n",
    "\n",
    "## Note: This will trigger human interaction after 3 iterations\n",
    "# For demonstration, you can use max_iterations=2 to avoid human interaction\n",
    "final_state = await run_dynamic_research(test_question, max_iterations=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Dynamic Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ‰ DYNAMIC RESEARCH COMPLETED\n",
      "================================================================================\n",
      "\n",
      "ğŸ“„ FINAL ANSWER:\n",
      "----------------------------------------\n",
      "ï½‰\n",
      "<think>\n",
      "Okay, let's tackle this query. The user is asking for a comprehensive answer based on the provided documents. The main question is about comparing the performance of their model and GPT-4V in hazard prediction, specifically in the context of intelligent vehicles. \n",
      "\n",
      "First, I need to parse through the documents to extract relevant information. Let me start by reviewing each document's content.\n",
      "\n",
      "Document 3 has examples of annotations and model outputs. For instance, in example (a), the annotation says Entity #1 turns too tight, hitting the car. The model's response mentions Entity #1 turning tight to the left, causing a collision due to being in the blind spot. GPT-4V's output includes speed details and mentions Entity #1 entering the path. In example (b), the annotation talks about Entity #1 stopping for Entity #2, leading to a collision. The model's response is about Entity #1 stopping due to traffic and Entity #2 crossing, leading to a crash. GPT-4V's response mentions a speed over 75 km/h, Entity #1 as a stationary vehicle, and Entity #2 as a bus merging, leading to a squeeze.\n",
      "\n",
      "Document 4 discusses the models' behaviors. The model is fine-tuned, follows instructions, but oversimplifies. GPT-4V is good at descriptive reasoning but struggles with entity recognition and contextual understanding. It also mentions GPT-4V misidentifying a white truck as a pedestrian and failing to recognize traffic signs.\n",
      "\n",
      "Document 5 talks about LLaVA's underperformance, possibly due to freezing the visual encoder and simple transformations. GPT-4 scores align with traditional metrics but offer a more nuanced measure of meaning.\n",
      "\n",
      "Putting this together, the user wants a structured answer that addresses their question, incorporates all findings, is clear, and includes specific examples. The answer should compare the two models, highlight strengths and weaknesses, and mention the evaluation metrics.\n",
      "\n",
      "I need to make sure to mention the examples from Document 3, the specific issues with GPT-4V like entity recognition and traffic signs, and the comparison with LLaVA. Also, the GPT-4 scores and their implications. The answer should be comprehensive, so I'll structure it into sections like model performance, strengths and weaknesses, evaluation metrics, and future directions. Including the examples will make it specific and informative.\n",
      "</think>\n",
      "\n",
      "### Comprehensive Analysis of Model Performance in Hazard Prediction for Intelligent Vehicles\n",
      "\n",
      "#### **1. Direct Address to the User's Question**  \n",
      "The user seeks a comparison of the performance of their model (a fine-tuned system) and **GPT-4V** (a large vision-language model) in hazard prediction tasks for intelligent vehicles. This analysis integrates findings from the provided documents to evaluate their strengths, weaknesses, and contextual accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Key Findings from the Research**\n",
      "\n",
      "#### **A. Model Performance in Hazard Prediction**  \n",
      "**Example (a):**  \n",
      "- **Annotation:** Entity #1 turns too tight, hitting the car.  \n",
      "- **Our Model:** Correctly identifies Entity #1â€™s blindspot as the cause of the collision.  \n",
      "- **GPT-4V:** Adds contextual details (e.g., speed of 45 km/h) but focuses on Entity #1â€™s unexpected entry into the carâ€™s path.  \n",
      "\n",
      "**Example (b):**  \n",
      "- **Annotation:** Entity #1 stops for Entity #2, leading to a collision.  \n",
      "- **Our Model:** Describes Entity #1â€™s braking due to traffic and Entity #2â€™s lane crossing, resulting in a crash.  \n",
      "- **GPT-4V:** Mentions a speed of over 75 km/h, misidentifies Entity #1 as a stationary vehicle, and incorrectly labels Entity #2 as a bus merging into the lane.  \n",
      "\n",
      "**Key Insight:**  \n",
      "- **Our Model** excels at **following structured instruction formats** and producing **consistent, patterned outputs**, but it **oversimplifies** hazard explanations.  \n",
      "- **GPT-4V** provides **descriptive, reasoning-based explanations** but struggles with **entity recognition** (e.g., misclassifying a white truck as a pedestrian) and **contextual understanding** (e.g., failing to link Entity #1â€™s actions to Entity #2â€™s response in example (b)).\n",
      "\n",
      "---\n",
      "\n",
      "#### **B. Strengths and Weaknesses of Each Model**  \n",
      "| **Aspect**               | **Our Model**                                                                 | **GPT-4V**                                                                 |\n",
      "|--------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------|\n",
      "| **Instruction Following** | Strong; produces consistent, structured outputs.                              | Strong; generates detailed, reasoning-based explanations.                |\n",
      "| **Entity Recognition**   | Limited; oversimplifies hazard explanations.                                  | Weak; misidentifies objects (e.g., white truck as pedestrian).           |\n",
      "| **Contextual Understanding** | Limited; lacks depth in causal relationships between entities.              | Weak; fails to capture nuanced interactions (e.g., Entity #1 â†’ Entity #2). |\n",
      "| **Traffic Sign/Signal Recognition** | Poor; does not explicitly address traffic rules.                            | Poor; frequently fails to recognize traffic signs and signals.           |\n",
      "| **Speed and Detail**     | Concise; focuses on core causes of collisions.                               | Detailed; includes speed, environmental context, and speculative scenarios. |\n",
      "\n",
      "---\n",
      "\n",
      "#### **C. Evaluation Metrics and Comparative Performance**  \n",
      "- **Traditional Metrics:**  \n",
      "  - **LLaVA (v1.5):** Underperforms due to its frozen visual encoder and simplistic image-text alignment, despite visual instruction tuning.  \n",
      "  - **Our Model vs. GPT-4V:** Traditional metrics (e.g., formal similarity) align with GPT-4 scores, but **GPT-4 scores** (56â€“58%) offer a **nuanced measure** of semantic closeness to annotations.  \n",
      "\n",
      "- **GPT-4 Scores:**  \n",
      "  - **Our Model:** Scores 56â€“58% on GPT-4 evaluations, indicating reasonable semantic alignment with annotations.  \n",
      "  - **LLaVA:** Scores similarly, but its performance is limited by architectural constraints (e.g., frozen visual encoder).  \n",
      "  - **GPT-4V:** Scores are not explicitly provided, but its **descriptive accuracy** is offset by **entity recognition errors**.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Specific Examples and Implications**  \n",
      "- **Example (a):**  \n",
      "  - **Our Model:** Correctly identifies the blindspot as the cause.  \n",
      "  - **GPT-4V:** Adds speed context but misses the blindspot detail.  \n",
      "  - **Implication:** GPT-4Vâ€™s focus on environmental factors (e.g., speed) may overlook critical spatial relationships.  \n",
      "\n",
      "- **Example (b):**  \n",
      "  - **Our Model:** Links Entity #1â€™s braking to traffic congestion and Entity #2â€™s lane crossing.  \n",
      "  - **GPT-4V:** Misidentifies Entity #1 as stationary and Entity #2 as a bus, leading to an incorrect \"squeeze\" scenario.  \n",
      "  - **Implication:** GPT-4Vâ€™s failure to recognize dynamic interactions (e.g., Entity #1â€™s braking causing Entity #2â€™s movement) highlights its limitations in causal reasoning.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Limitations and Future Directions**  \n",
      "- **Shared Limitations:**  \n",
      "  - Both models struggle with **spatial-temporal understanding** of traffic scenes.  \n",
      "  - **Entity recognition** and **contextual reasoning** remain critical challenges.  \n",
      "\n",
      "- **Future Research Directions:**  \n",
      "  - **Enhanced Visual-Text Alignment:** Improve GPT-4Vâ€™s ability to extract and interpret information from color-highlighted boxes.  \n",
      "  - **Fine-Tuning for Causal Reasoning:** Train models to explicitly model interactions between entities (e.g., Entity #1 â†’ Entity #2).  \n",
      "  - **Hybrid Architectures:** Combine the structured outputs of fine-tuned models with GPT-4Vâ€™s descriptive reasoning capabilities.  \n",
      "\n",
      "---\n",
      "\n",
      "### **5. Conclusion**  \n",
      "- **Our Model** excels in structured, rule-based hazard prediction but lacks depth in causal explanations.  \n",
      "- **GPT-4V** provides rich, descriptive insights but suffers from entity recognition errors and contextual misunderstandings.  \n",
      "- **GPT-4 Scores** offer a more nuanced evaluation of semantic alignment, suggesting that **semantic accuracy** is a critical metric for intelligent vehicle systems.  \n",
      "- **Collaborative approaches** combining fine-tuning and large vision-language models may yield the best results for real-world hazard prediction.  \n",
      "\n",
      "This analysis underscores the need for **domain-specific training** and **enhanced contextual understanding** to improve safety-critical applications in intelligent vehicles.\n",
      "\n",
      "================================================================================\n",
      "ğŸ§  DYNAMIC QUERY GENERATION TRACKING\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ User Question: Tell me the comparison results between GPT-4V and ours model and give an example as well.\n",
      "ğŸ”„ Total Iterations: 0\n",
      "\n",
      "ğŸ¯ Iteration 1 Reasoning:\n",
      "   Iteration 1: Generated 4 queries based on user question and previous findings\n",
      "\n",
      "ğŸ” ALL DYNAMIC QUERIES GENERATED:\n",
      "   1. Iteration 1: What are the performance differences between GPT-4V and our AI model?\n",
      "   2. Iteration 1: How does GPT-4V compare to our model in terms of image recognition and text generation?\n",
      "   3. Iteration 1: What are the key capabilities and limitations of GPT-4V versus our model, with an example?\n",
      "   4. Iteration 1: Can you provide a real-world example comparing GPT-4V's performance with our model in a specific task?\n",
      "\n",
      "ğŸ“Š Total Dynamic Queries: 4\n",
      "\n",
      "ğŸ“‹ DECISION LOG:\n",
      "   1. Generated 4 queries for iteration 1\n",
      "   2. Executed research with 4 queries\n",
      "   3. Decision: human_interaction - ğŸ§ª TESTING: Triggering human-in-the-loop after 1 iteration (set to trigger after 1 for testing)\n",
      "   4. Human chose to synthesize final answer\n",
      "   5. Final answer synthesized\n",
      "\n",
      "ğŸ”„ STATE TRANSITIONS:\n",
      "   1. query_generation -> research_execution\n",
      "   2. research_execution -> analysis\n",
      "   3. analysis -> human_interaction\n",
      "   4. human_interaction -> synthesis\n",
      "   5. synthesis -> end\n",
      "\n",
      "âœ… NO INFORMATION GAPS - Research Complete!\n",
      "\n",
      "ğŸ“ˆ FINAL METRICS:\n",
      "   Total RAG findings: 1\n",
      "   Total web findings: 0\n",
      "   Final answer length: 8499 characters\n",
      "   Confidence score: 0.8\n",
      "\n",
      "ğŸ¯ KEY ACHIEVEMENT:\n",
      "   âœ… ZERO hardcoded queries\n",
      "   âœ… FULLY dynamic query generation\n",
      "   âœ… State-driven research decisions\n",
      "   âœ… Intelligent gap analysis\n",
      "   âœ… Adaptive research strategy\n",
      "---------------\n",
      "Final Answer: ï½‰\n",
      "<think>\n",
      "Okay, let's tackle this query. The user is asking for a comprehensive answer based on the provided documents. The main question is about comparing the performance of their model and GPT-4V in hazard prediction, specifically in the context of intelligent vehicles. \n",
      "\n",
      "First, I need to parse through the documents to extract relevant information. Let me start by reviewing each document's content.\n",
      "\n",
      "Document 3 has examples of annotations and model outputs. For instance, in example (a), the annotation says Entity #1 turns too tight, hitting the car. The model's response mentions Entity #1 turning tight to the left, causing a collision due to being in the blind spot. GPT-4V's output includes speed details and mentions Entity #1 entering the path. In example (b), the annotation talks about Entity #1 stopping for Entity #2, leading to a collision. The model's response is about Entity #1 stopping due to traffic and Entity #2 crossing, leading to a crash. GPT-4V's response mentions a speed over 75 km/h, Entity #1 as a stationary vehicle, and Entity #2 as a bus merging, leading to a squeeze.\n",
      "\n",
      "Document 4 discusses the models' behaviors. The model is fine-tuned, follows instructions, but oversimplifies. GPT-4V is good at descriptive reasoning but struggles with entity recognition and contextual understanding. It also mentions GPT-4V misidentifying a white truck as a pedestrian and failing to recognize traffic signs.\n",
      "\n",
      "Document 5 talks about LLaVA's underperformance, possibly due to freezing the visual encoder and simple transformations. GPT-4 scores align with traditional metrics but offer a more nuanced measure of meaning.\n",
      "\n",
      "Putting this together, the user wants a structured answer that addresses their question, incorporates all findings, is clear, and includes specific examples. The answer should compare the two models, highlight strengths and weaknesses, and mention the evaluation metrics.\n",
      "\n",
      "I need to make sure to mention the examples from Document 3, the specific issues with GPT-4V like entity recognition and traffic signs, and the comparison with LLaVA. Also, the GPT-4 scores and their implications. The answer should be comprehensive, so I'll structure it into sections like model performance, strengths and weaknesses, evaluation metrics, and future directions. Including the examples will make it specific and informative.\n",
      "</think>\n",
      "\n",
      "### Comprehensive Analysis of Model Performance in Hazard Prediction for Intelligent Vehicles\n",
      "\n",
      "#### **1. Direct Address to the User's Question**  \n",
      "The user seeks a comparison of the performance of their model (a fine-tuned system) and **GPT-4V** (a large vision-language model) in hazard prediction tasks for intelligent vehicles. This analysis integrates findings from the provided documents to evaluate their strengths, weaknesses, and contextual accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Key Findings from the Research**\n",
      "\n",
      "#### **A. Model Performance in Hazard Prediction**  \n",
      "**Example (a):**  \n",
      "- **Annotation:** Entity #1 turns too tight, hitting the car.  \n",
      "- **Our Model:** Correctly identifies Entity #1â€™s blindspot as the cause of the collision.  \n",
      "- **GPT-4V:** Adds contextual details (e.g., speed of 45 km/h) but focuses on Entity #1â€™s unexpected entry into the carâ€™s path.  \n",
      "\n",
      "**Example (b):**  \n",
      "- **Annotation:** Entity #1 stops for Entity #2, leading to a collision.  \n",
      "- **Our Model:** Describes Entity #1â€™s braking due to traffic and Entity #2â€™s lane crossing, resulting in a crash.  \n",
      "- **GPT-4V:** Mentions a speed of over 75 km/h, misidentifies Entity #1 as a stationary vehicle, and incorrectly labels Entity #2 as a bus merging into the lane.  \n",
      "\n",
      "**Key Insight:**  \n",
      "- **Our Model** excels at **following structured instruction formats** and producing **consistent, patterned outputs**, but it **oversimplifies** hazard explanations.  \n",
      "- **GPT-4V** provides **descriptive, reasoning-based explanations** but struggles with **entity recognition** (e.g., misclassifying a white truck as a pedestrian) and **contextual understanding** (e.g., failing to link Entity #1â€™s actions to Entity #2â€™s response in example (b)).\n",
      "\n",
      "---\n",
      "\n",
      "#### **B. Strengths and Weaknesses of Each Model**  \n",
      "| **Aspect**               | **Our Model**                                                                 | **GPT-4V**                                                                 |\n",
      "|--------------------------|-------------------------------------------------------------------------------|----------------------------------------------------------------------------|\n",
      "| **Instruction Following** | Strong; produces consistent, structured outputs.                              | Strong; generates detailed, reasoning-based explanations.                |\n",
      "| **Entity Recognition**   | Limited; oversimplifies hazard explanations.                                  | Weak; misidentifies objects (e.g., white truck as pedestrian).           |\n",
      "| **Contextual Understanding** | Limited; lacks depth in causal relationships between entities.              | Weak; fails to capture nuanced interactions (e.g., Entity #1 â†’ Entity #2). |\n",
      "| **Traffic Sign/Signal Recognition** | Poor; does not explicitly address traffic rules.                            | Poor; frequently fails to recognize traffic signs and signals.           |\n",
      "| **Speed and Detail**     | Concise; focuses on core causes of collisions.                               | Detailed; includes speed, environmental context, and speculative scenarios. |\n",
      "\n",
      "---\n",
      "\n",
      "#### **C. Evaluation Metrics and Comparative Performance**  \n",
      "- **Traditional Metrics:**  \n",
      "  - **LLaVA (v1.5):** Underperforms due to its frozen visual encoder and simplistic image-text alignment, despite visual instruction tuning.  \n",
      "  - **Our Model vs. GPT-4V:** Traditional metrics (e.g., formal similarity) align with GPT-4 scores, but **GPT-4 scores** (56â€“58%) offer a **nuanced measure** of semantic closeness to annotations.  \n",
      "\n",
      "- **GPT-4 Scores:**  \n",
      "  - **Our Model:** Scores 56â€“58% on GPT-4 evaluations, indicating reasonable semantic alignment with annotations.  \n",
      "  - **LLaVA:** Scores similarly, but its performance is limited by architectural constraints (e.g., frozen visual encoder).  \n",
      "  - **GPT-4V:** Scores are not explicitly provided, but its **descriptive accuracy** is offset by **entity recognition errors**.  \n",
      "\n",
      "---\n",
      "\n",
      "### **3. Specific Examples and Implications**  \n",
      "- **Example (a):**  \n",
      "  - **Our Model:** Correctly identifies the blindspot as the cause.  \n",
      "  - **GPT-4V:** Adds speed context but misses the blindspot detail.  \n",
      "  - **Implication:** GPT-4Vâ€™s focus on environmental factors (e.g., speed) may overlook critical spatial relationships.  \n",
      "\n",
      "- **Example (b):**  \n",
      "  - **Our Model:** Links Entity #1â€™s braking to traffic congestion and Entity #2â€™s lane crossing.  \n",
      "  - **GPT-4V:** Misidentifies Entity #1 as stationary and Entity #2 as a bus, leading to an incorrect \"squeeze\" scenario.  \n",
      "  - **Implication:** GPT-4Vâ€™s failure to recognize dynamic interactions (e.g., Entity #1â€™s braking causing Entity #2â€™s movement) highlights its limitations in causal reasoning.  \n",
      "\n",
      "---\n",
      "\n",
      "### **4. Limitations and Future Directions**  \n",
      "- **Shared Limitations:**  \n",
      "  - Both models struggle with **spatial-temporal understanding** of traffic scenes.  \n",
      "  - **Entity recognition** and **contextual reasoning** remain critical challenges.  \n",
      "\n",
      "- **Future Research Directions:**  \n",
      "  - **Enhanced Visual-Text Alignment:** Improve GPT-4Vâ€™s ability to extract and interpret information from color-highlighted boxes.  \n",
      "  - **Fine-Tuning for Causal Reasoning:** Train models to explicitly model interactions between entities (e.g., Entity #1 â†’ Entity #2).  \n",
      "  - **Hybrid Architectures:** Combine the structured outputs of fine-tuned models with GPT-4Vâ€™s descriptive reasoning capabilities.  \n",
      "\n",
      "---\n",
      "\n",
      "### **5. Conclusion**  \n",
      "- **Our Model** excels in structured, rule-based hazard prediction but lacks depth in causal explanations.  \n",
      "- **GPT-4V** provides rich, descriptive insights but suffers from entity recognition errors and contextual misunderstandings.  \n",
      "- **GPT-4 Scores** offer a more nuanced evaluation of semantic alignment, suggesting that **semantic accuracy** is a critical metric for intelligent vehicle systems.  \n",
      "- **Collaborative approaches** combining fine-tuning and large vision-language models may yield the best results for real-world hazard prediction.  \n",
      "\n",
      "This analysis underscores the need for **domain-specific training** and **enhanced contextual understanding** to improve safety-critical applications in intelligent vehicles.\n"
     ]
    }
   ],
   "source": [
    "if final_state:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ‰ DYNAMIC RESEARCH COMPLETED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Show final answer\n",
    "    print(\"\\nğŸ“„ FINAL ANSWER:\")\n",
    "    print(\"-\"*40)\n",
    "    print(final_state[\"synthesized_answer\"])\n",
    "    \n",
    "    # Show DYNAMIC query generation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ğŸ§  DYNAMIC QUERY GENERATION TRACKING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ User Question: {final_state['user_question']}\")\n",
    "    print(f\"ğŸ”„ Total Iterations: {final_state['research_iteration'] - 1}\")\n",
    "    \n",
    "    for i, reasoning in enumerate(final_state['query_generation_reasoning'], 1):\n",
    "        print(f\"\\nğŸ¯ Iteration {i} Reasoning:\")\n",
    "        print(f\"   {reasoning}\")\n",
    "    \n",
    "    # Show all DYNAMIC queries\n",
    "    print(f\"\\nğŸ” ALL DYNAMIC QUERIES GENERATED:\")\n",
    "    all_queries = []\n",
    "    for finding in final_state['rag_findings']:\n",
    "        iteration = finding['iteration']\n",
    "        for query in finding['queries']:\n",
    "            all_queries.append(f\"Iteration {iteration}: {query}\")\n",
    "    \n",
    "    for i, query in enumerate(all_queries, 1):\n",
    "        print(f\"   {i}. {query}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Total Dynamic Queries: {len(all_queries)}\")\n",
    "    \n",
    "    # Show decision log\n",
    "    print(f\"\\nğŸ“‹ DECISION LOG:\")\n",
    "    for i, decision in enumerate(final_state['decision_log'], 1):\n",
    "        print(f\"   {i}. {decision}\")\n",
    "    \n",
    "    # Show state transitions\n",
    "    print(f\"\\nğŸ”„ STATE TRANSITIONS:\")\n",
    "    for i, transition in enumerate(final_state['state_transitions'], 1):\n",
    "        print(f\"   {i}. {transition}\")\n",
    "    \n",
    "    # Show information gaps\n",
    "    if final_state['information_gaps']:\n",
    "        print(f\"\\nğŸ•³ï¸ INFORMATION GAPS IDENTIFIED:\")\n",
    "        for gap in final_state['information_gaps']:\n",
    "            print(f\"   â€¢ {gap}\")\n",
    "    else:\n",
    "        print(f\"\\nâœ… NO INFORMATION GAPS - Research Complete!\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ FINAL METRICS:\")\n",
    "    print(f\"   Total RAG findings: {len(final_state['rag_findings'])}\")\n",
    "    print(f\"   Total web findings: {len(final_state['web_findings'])}\")\n",
    "    print(f\"   Final answer length: {len(final_state['synthesized_answer'])} characters\")\n",
    "    print(f\"   Confidence score: {final_state['confidence_score']}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ KEY ACHIEVEMENT:\")\n",
    "    print(f\"   âœ… ZERO hardcoded queries\")\n",
    "    print(f\"   âœ… FULLY dynamic query generation\")\n",
    "    print(f\"   âœ… State-driven research decisions\")\n",
    "    print(f\"   âœ… Intelligent gap analysis\")\n",
    "    print(f\"   âœ… Adaptive research strategy\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No final state to display\")\n",
    "\n",
    "print(\"---------------\")\n",
    "print(\"Final Answer:\", final_state['synthesized_answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.similarity_search(query, k=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
